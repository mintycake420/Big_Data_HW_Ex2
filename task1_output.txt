[INFO] Scanning for projects...
[INFO] 
[INFO] ---------------< com.haifa.bigdata:spark-imdb-analysis >----------------
[INFO] Building spark-imdb-analysis 1.0-SNAPSHOT
[INFO]   from pom.xml
[INFO] --------------------------------[ jar ]---------------------------------
[WARNING] 2 problems were encountered while building the effective model for org.apache.yetus:audience-annotations:jar:0.5.0 during dependency collection step for project (use -X to see details)
[INFO] 
[INFO] --- exec:3.1.0:java (default-cli) @ spark-imdb-analysis ---
=== Task 1: Data Cleaning and Preprocessing ===

Original dataset schema:
root
 |-- title: string (nullable = true)
 |-- year: string (nullable = true)
 |-- certificate: string (nullable = true)
 |-- duration: string (nullable = true)
 |-- genre: string (nullable = true)
 |-- rating: double (nullable = true)
 |-- description: string (nullable = true)
 |-- stars: string (nullable = true)
 |-- votes: string (nullable = true)


Original dataset count: 9957

Sample of original data:
+----------------------+-----------+-----------+--------+----------------------------+------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+-------+
|title                 |year       |certificate|duration|genre                       |rating|description                                                                                                                                                                                                                   |stars                                                                           |votes  |
+----------------------+-----------+-----------+--------+----------------------------+------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+-------+
|Cobra Kai             |(2018– )   |TV-14      |30 min  |Action, Comedy, Drama       |8.5   |Decades after their 1984 All Valley Karate Tournament bout, a middle-aged Daniel LaRusso and Johnny Lawrence find themselves martial-arts rivals again.                                                                       |['Ralph Macchio, ', 'William Zabka, ', 'Courtney Henggeler, ', 'Xolo Maridueña']|177,031|
|The Crown             |(2016– )   |TV-MA      |58 min  |Biography, Drama, History   |8.7   |Follows the political rivalries and romance of Queen Elizabeth II's reign and the events that shaped the second half of the twentieth century.                                                                                |['Claire Foy, ', 'Olivia Colman, ', 'Imelda Staunton, ', 'Matt Smith']          |199,885|
|Better Call Saul      |(2015–2022)|TV-MA      |46 min  |Crime, Drama                |8.9   |The trials and tribulations of criminal lawyer Jimmy McGill before his fateful run-in with Walter White and Jesse Pinkman.                                                                                                    |['Bob Odenkirk, ', 'Rhea Seehorn, ', 'Jonathan Banks, ', 'Patrick Fabian']      |501,384|
|Devil in Ohio         |(2022)     |TV-MA      |356 min |Drama, Horror, Mystery      |5.9   |When a psychiatrist shelters a mysterious cult escapee, her world is turned upside down as the girl's arrival threatens to tear her own family apart.                                                                         |['Emily Deschanel, ', 'Sam Jaeger, ', 'Gerardo Celasco, ', 'Madeleine Arthur']  |9,773  |
|Cyberpunk: Edgerunners|(2022– )   |TV-MA      |24 min  |Animation, Action, Adventure|8.6   |A Street Kid trying to survive in a technology and body modification-obsessed city of the future. Having everything to lose, he chooses to stay alive by becoming an Edgerunner, a Mercenary outlaw also known as a Cyberpunk.|['Zach Aguilar, ', 'Kenichiro Ohashi, ', 'Emi Lo, ', 'Aoi Yûki']                |15,413 |
+----------------------+-----------+-----------+--------+----------------------------+------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+-------+
only showing top 5 rows


--- Step 1: Handling Missing Values ---

Missing values count per column:
title: 0
year: 527
certificate: 3453
duration: 2036
genre: 73
rating: 1173
description: 0
stars: 0
votes: 1173

After filtering missing critical values: 0

--- Step 2: Converting Votes to Numeric ---

Sample of votes conversion:
+-----+-----+-------------+
|title|votes|votes_numeric|
+-----+-----+-------------+
+-----+-----+-------------+


--- Step 3: Extracting Year Values ---

Sample of year extraction:
+-----+----+----------+----------+
|title|year|start_year|is_tv_show|
+-----+----+----------+----------+
+-----+----+----------+----------+


=== Cleaned Dataset Summary ===
Final row count: 0

Cleaned dataset schema:
root
 |-- title: string (nullable = true)
 |-- year: string (nullable = true)
 |-- certificate: string (nullable = true)
 |-- duration: string (nullable = true)
 |-- genre: string (nullable = true)
 |-- rating: double (nullable = true)
 |-- description: string (nullable = true)
 |-- stars: string (nullable = true)
 |-- votes: string (nullable = true)
 |-- votes_numeric: long (nullable = true)
 |-- year_clean: string (nullable = true)
 |-- start_year: integer (nullable = true)
 |-- is_tv_show: boolean (nullable = true)
 |-- rating_numeric: double (nullable = true)


Rating statistics:
+-------+--------------+
|summary|rating_numeric|
+-------+--------------+
|  count|             0|
|   mean|          NULL|
| stddev|          NULL|
|    min|          NULL|
|    max|          NULL|
+-------+--------------+


Votes statistics:
+-------+-------------+
|summary|votes_numeric|
+-------+-------------+
|  count|            0|
|   mean|         NULL|
| stddev|         NULL|
|    min|         NULL|
|    max|         NULL|
+-------+-------------+


Year statistics:
+-------+----------+
|summary|start_year|
+-------+----------+
|  count|         0|
|   mean|      NULL|
| stddev|      NULL|
|    min|      NULL|
|    max|      NULL|
+-------+----------+


TV Shows vs Movies:
+----------+-----+
|is_tv_show|total|
+----------+-----+
+----------+-----+


Saving cleaned dataset to: output/cleaned_imdb_data
[WARNING] 
java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: Hadoop home directory C:hadoop is not an absolute path. -see https://wiki.apache.org/hadoop/WindowsProblems
    at org.apache.hadoop.util.Shell.getWinUtilsPath (Shell.java:735)
    at org.apache.hadoop.util.Shell.getSetPermissionCommand (Shell.java:270)
    at org.apache.hadoop.util.Shell.getSetPermissionCommand (Shell.java:286)
    at org.apache.hadoop.fs.RawLocalFileSystem.setPermission (RawLocalFileSystem.java:978)
    at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode (RawLocalFileSystem.java:660)
    at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission (RawLocalFileSystem.java:700)
    at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs (RawLocalFileSystem.java:672)
    at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission (RawLocalFileSystem.java:699)
    at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs (RawLocalFileSystem.java:672)
    at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission (RawLocalFileSystem.java:699)
    at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs (RawLocalFileSystem.java:672)
    at org.apache.hadoop.fs.ChecksumFileSystem.mkdirs (ChecksumFileSystem.java:788)
    at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob (FileOutputCommitter.java:356)
    at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob (HadoopMapReduceCommitProtocol.scala:188)
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit (FileFormatWriter.scala:269)
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite (FileFormatWriter.scala:304)
    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write (FileFormatWriter.scala:190)
    at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run (InsertIntoHadoopFsRelationCommand.scala:190)
    at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute (commands.scala:113)
    at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult (commands.scala:111)
    at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect (commands.scala:125)
    at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1 (QueryExecution.scala:107)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6 (SQLExecution.scala:125)
    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated (SQLExecution.scala:201)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1 (SQLExecution.scala:108)
    at org.apache.spark.sql.SparkSession.withActive (SparkSession.scala:900)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId (SQLExecution.scala:66)
    at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse (QueryExecution.scala:107)
    at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse (QueryExecution.scala:98)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1 (TreeNode.scala:461)
    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin (origin.scala:76)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning (TreeNode.scala:461)
    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning (LogicalPlan.scala:32)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning (AnalysisHelper.scala:267)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$ (AnalysisHelper.scala:263)
    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning (LogicalPlan.scala:32)
    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning (LogicalPlan.scala:32)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown (TreeNode.scala:437)
    at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands (QueryExecution.scala:98)
    at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute (QueryExecution.scala:85)
    at org.apache.spark.sql.execution.QueryExecution.commandExecuted (QueryExecution.scala:83)
    at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted (QueryExecution.scala:142)
    at org.apache.spark.sql.DataFrameWriter.runCommand (DataFrameWriter.scala:869)
    at org.apache.spark.sql.DataFrameWriter.saveToV1Source (DataFrameWriter.scala:391)
    at org.apache.spark.sql.DataFrameWriter.saveInternal (DataFrameWriter.scala:364)
    at org.apache.spark.sql.DataFrameWriter.save (DataFrameWriter.scala:243)
    at org.apache.spark.sql.DataFrameWriter.csv (DataFrameWriter.scala:860)
    at Task1_DataCleaning.main (Task1_DataCleaning.java:125)
    at org.codehaus.mojo.exec.ExecJavaMojo$1.run (ExecJavaMojo.java:279)
    at java.lang.Thread.run (Thread.java:1570)
Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: Hadoop home directory C:hadoop is not an absolute path. -see https://wiki.apache.org/hadoop/WindowsProblems
    at org.apache.hadoop.util.Shell.fileNotFoundException (Shell.java:547)
    at org.apache.hadoop.util.Shell.getHadoopHomeDir (Shell.java:568)
    at org.apache.hadoop.util.Shell.getQualifiedBin (Shell.java:591)
    at org.apache.hadoop.util.Shell.<clinit> (Shell.java:688)
    at org.apache.hadoop.util.StringUtils.<clinit> (StringUtils.java:79)
    at org.apache.hadoop.conf.Configuration.getBoolean (Configuration.java:1712)
    at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal (SecurityUtil.java:99)
    at org.apache.hadoop.security.SecurityUtil.<clinit> (SecurityUtil.java:88)
    at org.apache.hadoop.security.UserGroupInformation.initialize (UserGroupInformation.java:312)
    at org.apache.hadoop.security.UserGroupInformation.ensureInitialized (UserGroupInformation.java:300)
    at org.apache.hadoop.security.UserGroupInformation.getCurrentUser (UserGroupInformation.java:575)
    at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1 (Utils.scala:2417)
    at scala.Option.getOrElse (Option.scala:189)
    at org.apache.spark.util.Utils$.getCurrentUserName (Utils.scala:2417)
    at org.apache.spark.SparkContext.<init> (SparkContext.scala:329)
    at org.apache.spark.SparkContext$.getOrCreate (SparkContext.scala:2883)
    at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2 (SparkSession.scala:1099)
    at scala.Option.getOrElse (Option.scala:189)
    at org.apache.spark.sql.SparkSession$Builder.getOrCreate (SparkSession.scala:1093)
    at Task1_DataCleaning.main (Task1_DataCleaning.java:23)
    at org.codehaus.mojo.exec.ExecJavaMojo$1.run (ExecJavaMojo.java:279)
    at java.lang.Thread.run (Thread.java:1570)
Caused by: java.io.FileNotFoundException: Hadoop home directory C:hadoop is not an absolute path.
    at org.apache.hadoop.util.Shell.checkHadoopHomeInner (Shell.java:486)
    at org.apache.hadoop.util.Shell.checkHadoopHome (Shell.java:438)
    at org.apache.hadoop.util.Shell.<clinit> (Shell.java:515)
    at org.apache.hadoop.util.StringUtils.<clinit> (StringUtils.java:79)
    at org.apache.hadoop.conf.Configuration.getBoolean (Configuration.java:1712)
    at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal (SecurityUtil.java:99)
    at org.apache.hadoop.security.SecurityUtil.<clinit> (SecurityUtil.java:88)
    at org.apache.hadoop.security.UserGroupInformation.initialize (UserGroupInformation.java:312)
    at org.apache.hadoop.security.UserGroupInformation.ensureInitialized (UserGroupInformation.java:300)
    at org.apache.hadoop.security.UserGroupInformation.getCurrentUser (UserGroupInformation.java:575)
    at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1 (Utils.scala:2417)
    at scala.Option.getOrElse (Option.scala:189)
    at org.apache.spark.util.Utils$.getCurrentUserName (Utils.scala:2417)
    at org.apache.spark.SparkContext.<init> (SparkContext.scala:329)
    at org.apache.spark.SparkContext$.getOrCreate (SparkContext.scala:2883)
    at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2 (SparkSession.scala:1099)
    at scala.Option.getOrElse (Option.scala:189)
    at org.apache.spark.sql.SparkSession$Builder.getOrCreate (SparkSession.scala:1093)
    at Task1_DataCleaning.main (Task1_DataCleaning.java:23)
    at org.codehaus.mojo.exec.ExecJavaMojo$1.run (ExecJavaMojo.java:279)
    at java.lang.Thread.run (Thread.java:1570)
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  14.451 s
[INFO] Finished at: 2026-02-06T22:25:06+02:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:3.1.0:java (default-cli) on project spark-imdb-analysis: An exception occurred while executing the Java class. java.io.FileNotFoundException: java.io.FileNotFoundException: Hadoop home directory C:hadoop is not an absolute path. -see https://wiki.apache.org/hadoop/WindowsProblems -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[WARNING] 
java.lang.NoClassDefFoundError: org/apache/hadoop/util/ShutdownHookManager$2
    at org.apache.hadoop.util.ShutdownHookManager.getShutdownHooksInOrder (ShutdownHookManager.java:275)
    at org.apache.hadoop.util.ShutdownHookManager.executeShutdown (ShutdownHookManager.java:121)
    at org.apache.hadoop.util.ShutdownHookManager$1.run (ShutdownHookManager.java:95)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.util.ShutdownHookManager$2
    at org.codehaus.mojo.exec.URLClassLoaderBuilder$ExecJavaClassLoader.loadClass (URLClassLoaderBuilder.java:198)
    at java.lang.ClassLoader.loadClass (ClassLoader.java:525)
    at org.apache.hadoop.util.ShutdownHookManager.getShutdownHooksInOrder (ShutdownHookManager.java:275)
    at org.apache.hadoop.util.ShutdownHookManager.executeShutdown (ShutdownHookManager.java:121)
    at org.apache.hadoop.util.ShutdownHookManager$1.run (ShutdownHookManager.java:95)
